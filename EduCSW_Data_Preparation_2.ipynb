{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Awesome align"
      ],
      "metadata": {
        "id": "8PfXy1Iu1ddg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.1.0\n",
        "import torch\n",
        "import transformers\n",
        "import itertools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bREWn1Et1pUk",
        "outputId": "ad647ecf-5b6c-4abd-8a52-ba60f6066239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (1.26.4)\n",
            "Collecting tokenizers==0.8.1.rc2 (from transformers==3.1.0)\n",
            "  Downloading tokenizers-0.8.1rc2.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (4.66.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (2024.9.11)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from transformers==3.1.0) (0.2.0)\n",
            "Collecting sacremoses (from transformers==3.1.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.1.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==3.1.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==3.1.0) (1.4.2)\n",
            "Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.0/884.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "6c7a3b224fab45a7a4773ca6d2b97ce7",
            "ba4765f7a58549558d09acd810e7bac4",
            "108f4a6ca96b4e2a9a340929dd9d08b4",
            "019666c5a3e548da9686e1217d0d8ed9",
            "df8b00a8bbe54e87b73c67b2332194b4",
            "f0792c3f97754c049ede470e774994f6",
            "9c230c5c39684bb48785a7eb0d7acd12",
            "cb84386b8efe4f5e964a441743d3c70f",
            "5b69891c8ee3464abb78d61766318566",
            "185ff8d36bf542ac8fa09644b73b8883",
            "d51854571dd44b22af65030c7159fc8e",
            "e6e4844c17f345b9a77fb7a9adf247af",
            "b888c7b99ccb48a89a1611c41309d1dd",
            "4fdb84176d4b411b9bacd17d629df573",
            "4a29019664d4413b8bb72a55f26e276f",
            "92d8aacf378840ec88918a2121be9671",
            "7e55fc6b41744ee994bcc83365eb3f90",
            "8748f20099224da88b8e4912ad137e09",
            "1216e307e8074682b2126fb12a5806ed",
            "a2df8e0c75044ee4a7c27c8328a4c5c9",
            "029cff542c434704a02017165004e917",
            "c11c8ba2d8484e9cbaeec7428350083f",
            "6a0302be9bf2429885107f3accd72231",
            "73497eba663d427cbe8b8a27e7cf847d",
            "7579761e95d24ac5a1b207200c6e35f9",
            "299936c1acda4271a5a72efe9ff6a3dc",
            "ae0f1969e6d442809f1bc7c8b737914c",
            "6467f29a85234528bd80fe34d8afa435",
            "eb93f56064574a1197157bfa495417d0",
            "6e7cd1924cf94870a84145cdddb6efa3",
            "55524a2a1ef847e99767682811ca085e",
            "bf25e6f01d094189b90b67d2e9f01194",
            "cb60a85764a94a8abb7f4795bd20c6db",
            "5a29399ffd32487392998a8bd71e7afc",
            "8e5e422b4cc841c3ae3512f9d7e38de4",
            "2533bba0c1b644c6b493737ea2cdfd43",
            "ea8958bffa2e4c8b827e139544045817",
            "d02a3e178c4e45a39fde36e6aed7e682",
            "639772f2afb64fb6b5785996c18f7c35",
            "9843f60289e34d8c8dc1ab3ad97bef32",
            "99924fc973c2400a953d83109fcb87b9",
            "99932ddbf9044206b2b3eb61ded71efa",
            "946e3e77eb444e728970e5df90684cef",
            "c40b06220eac4de4917f946a9da86b88",
            "6a59e7e0722e4888aa72a6d294fdd21c",
            "11a745a63caf422cad95c3e096a50acf",
            "533275eec6ae408795df96dd2340aa7f",
            "06a298f495164b80a387362bc972da09",
            "4940c84ee88f4eb18da0d16ed7de9597",
            "887277c0e37846a4b0f4190658217e5c",
            "3b681fb3d1474a65905b40dc93c0aefe",
            "129ba6bd20d642979b1f3860d5aa20c5",
            "bd038ecbeb5f4892a702e9d1bd63472d",
            "f6ed1338192e4028b56c58d7baac1ef6",
            "80de5374dabb463abbbd72a86b1c9da1"
          ]
        },
        "id": "IK468URP2Qn-",
        "outputId": "745d2431-9eb9-49e8-a038-057eff271724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c7a3b224fab45a7a4773ca6d2b97ce7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6e4844c17f345b9a77fb7a9adf247af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a0302be9bf2429885107f3accd72231"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a29399ffd32487392998a8bd71e7afc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a59e7e0722e4888aa72a6d294fdd21c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "\n",
        "def align_sentences(src, tgt, align_layer=8, threshold=1e-3):\n",
        "    # pre-processing\n",
        "    sent_src = list(jieba.cut(src.strip()))\n",
        "    sent_tgt = tgt.strip().split()\n",
        "\n",
        "    token_src = [tokenizer.tokenize(word) for word in sent_src]\n",
        "    token_tgt = [tokenizer.tokenize(word) for word in sent_tgt]\n",
        "\n",
        "    wid_src = [tokenizer.convert_tokens_to_ids(x) for x in token_src]\n",
        "    wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
        "\n",
        "    ids_src = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids']\n",
        "    ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
        "\n",
        "    sub2word_map_src = []\n",
        "    for i, word_list in enumerate(token_src):\n",
        "        sub2word_map_src += [i for _ in word_list]\n",
        "    sub2word_map_tgt = []\n",
        "    for i, word_list in enumerate(token_tgt):\n",
        "        sub2word_map_tgt += [i for _ in word_list]\n",
        "\n",
        "    # alignment\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "        out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "        dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
        "\n",
        "        softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
        "        softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
        "\n",
        "        softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
        "\n",
        "    align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
        "    align_words = set()\n",
        "    for i, j in align_subwords:\n",
        "        align_words.add((sub2word_map_src[i], sub2word_map_tgt[j]))\n",
        "\n",
        "    return align_words, sent_src, sent_tgt\n",
        "\n",
        "# Process your dataset and add results as a new column\n",
        "aligned_results = []\n",
        "for zh, en in zip(cs_lan_en[\"instruction_zh\"], cs_lan_en[\"instruction_en\"]):\n",
        "    alignment, zh_words, en_words = align_sentences(zh, en)\n",
        "    aligned_results.append({\n",
        "        'zh_words': zh_words,\n",
        "        'en_words': en_words,\n",
        "        'alignment': list(alignment)\n",
        "    })\n",
        "\n",
        "# Add the aligned_results as a new column to cs_lan_en\n",
        "cs_lan_en['alignment_info'] = aligned_results\n",
        "\n",
        "# Print a sample result to verify\n",
        "print(cs_lan_en['alignment_info'].iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aChOpZ3A2YOz",
        "outputId": "e1f241af-4b82-4e87-cd24-165cf1c7229f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'zh_words': ['什么', '是', '计算机', '？'], 'en_words': [\"What's\", 'a', 'computer?'], 'alignment': [(0, 0), (1, 1), (2, 2), (1, 0), (3, 2)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "cs_lan_en.to_csv('aligned_zh_en.csv', index=False)\n",
        "files.download('aligned_zh_en.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gY_nujKbDXZ-",
        "outputId": "5d881775-1302-4db7-9630-05f9617ac7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a39845c-a1a9-45e7-9322-af675a7f48f0\", \"aligned_zh_en.csv\", 331800)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}